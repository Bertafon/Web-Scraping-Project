{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0556a39e-524e-48bf-8aee-c628c1b5c42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7148515-223e-476c-a13a-47ad0c217616",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### part 1 list of all products\n",
    "\n",
    "# it is not possible to download links to products on the page using BS or Scrapy. It is necessary to use Selenium.\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "d = webdriver.Chrome(executable_path='chromedriver') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f60c5033-760c-4349-aed5-f64292b96fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.get(\"https://www.notino.pl/perfumy/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03153578-8d9d-4d0f-b23b-986ce76f1503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of the last page\n",
    "max_range = int(d.find_elements(\"xpath\",\"//span[@data-testid='page-item']\")[-1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98741794-0ab5-4d94-8827-26ae4e9af0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = [] # list of links to each product\n",
    "url = 'https://www.notino.pl/perfumy/?f=0-1-55544'\n",
    "for i in range(1,max_range+1): \n",
    "    url = url.replace(f'?f={i-1}',f'?f={i}') # next pages are created according to this scheme\n",
    "    d.get(url) \n",
    "    product = d.find_elements(\"xpath\",\"//div[@data-testid='product-container']//a\") # find links to products\n",
    "    sleep(2)\n",
    "    for i in product:\n",
    "        products.append(i.get_attribute('href')) # add links to the list\n",
    "products = list(set(products)) # unique values\n",
    "unnecessary_links = ['https://www.notino.pl/opakowanie-prezentowe/', 'https://www.notino.pl/mobile-application/', 'https://www.notino.pl/wyprzedaz-perfum-kosmetykow/']\n",
    "for link in unnecessary_links:\n",
    "    if link in products:\n",
    "        products.remove(link) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7af98857-7257-4729-87a9-4d1f22921aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37e7fd9d-5156-4077-b7bc-a8e1f8c26e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the list with all links to the csv file so you don't have to repeat the above every time\n",
    "df = pd.DataFrame(products)\n",
    "df.dropna(inplace=True)\n",
    "df.to_csv(\"link_list.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f19e04c2-5c62-41a5-ba45-56c93f6a4dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Part 2 - scraping each product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3d9d70a-2eaf-4ddd-98c2-7d11d5fa645f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import logging\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.linkextractors import LinkExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21bb0381-a4ef-429d-b934-fdcf176196ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductsSpider(scrapy.Spider):\n",
    "    \n",
    "    name = 'products'\n",
    "    start_urls = products\n",
    "            \n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'FEEDS': {'res3.csv': {'format':'csv'}}\n",
    "        ,'DOWNLOAD_DELAY': 3 \n",
    "        ,'RANDOMIZE_DOWNLOAD_DELAY' : True \n",
    "    }\n",
    "    \n",
    "    def parse_product(self,response):\n",
    "        \n",
    "        xpath_name = '//title/text()' \n",
    "        xpath_EAN = '//script[@type=\"application/ld+json\"]//text()' \n",
    "        xpath_price_promo = '//div[@id = \"pd-price\"]/span/@content'\n",
    "        xpath_price = '//span[@data-testid = \"originalPriceLineThroughWrapper\"]/span/span/@content'\n",
    "            \n",
    "        name = response.xpath(xpath_name).get()[:-12]\n",
    "        EAN = json.loads(response.xpath(xpath_EAN).get())['gtin13']\n",
    "        price_promo = response.xpath(xpath_price_promo).get()\n",
    "        price = response.xpath(xpath_price).get()\n",
    "        \n",
    "        yield {\n",
    "                'Name': name,\n",
    "                'EAN': EAN,\n",
    "                'price': price,\n",
    "                'price promo': price_promo,\n",
    "                'url': response.url\n",
    "                }\n",
    "    \n",
    "    def parse(self,response):\n",
    "        \n",
    "        # product can have many variants\n",
    "        xpath_variant = '//div[@id = \"pdVariantsTile\"]//@href' \n",
    "        \n",
    "        if response.xpath(xpath_variant).extract() == []: # if there is only one variant then scrape response.url\n",
    "            yield scrapy.Request(response.url, self.parse_product)\n",
    "            \n",
    "        else: # if there are more, generate a list of them and scrape everything\n",
    "            for url in response.xpath(xpath_variant).extract():\n",
    "                href = response.urljoin(url)\n",
    "                yield scrapy.Request(href, self.parse_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b61d3917-510e-4247-b366-8fa85ce28705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-22 21:13:25 [scrapy.utils.log] INFO: Scrapy 2.7.1 started (bot: scrapybot)\n",
      "2023-07-22 21:13:25 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.9.4 (tags/v3.9.4:1f2e308, Apr  6 2021, 13:40:21) [MSC v.1928 64 bit (AMD64)], pyOpenSSL 23.0.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 39.0.0, Platform Windows-10-10.0.19041-SP0\n",
      "2023-07-22 21:13:25 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 30,\n",
      " 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
      "               '(KHTML, like Gecko) Chrome/101.0.4951.34 Safari/537.36'}\n",
      "2023-07-22 21:13:25 [py.warnings] WARNING: c:\\users\\norbert\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\scrapy\\utils\\request.py:231: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.34 Safari/537.36'\n",
    "})\n",
    "\n",
    "process.crawl(ProductsSpider)\n",
    "process.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
